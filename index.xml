<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Darin Tsui</title>
    <link>https://darintsui.github.io/</link>
      <atom:link href="https://darintsui.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Darin Tsui</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://darintsui.github.io/media/icon_hu382b23d3a754463455488130ff2cf495_259114_512x512_fill_lanczos_center_3.png</url>
      <title>Darin Tsui</title>
      <link>https://darintsui.github.io/</link>
    </image>
    
    <item>
      <title>Supervised ML Approaches for Predicting Cancer Survival Rate</title>
      <link>https://darintsui.github.io/project/cancersurvival/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/project/cancersurvival/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&amp;ldquo;How long have I got?&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Most Stage IV lung cancer patients worldwide tend to have poor survival: 25-30% die within the first 3 months of diagnosis. However, of those patients suriving longer than 3 months, 10-15% of those patients survive very long. The question becomes then: what metrics can we use to determine a patient&amp;rsquo;s survival projection using clinically relevant data. In this project, we look to develop and validate a survival prediction model in a cohort of different stages patients of lung cancer.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the Cancer Genome Atlas Lung Adenocarcinoma (TCGA-LUAD). The data consists of 572 samples obtained from 512 individuals, with over 20,000 genes for each. We mainly utilize the gene expression and metadata datasets for our analysis. We performed Log2 transformation on the expression data and removed the recurrent and normal samples.&lt;/p&gt;
&lt;p&gt;We look to analyze two different clinical endpoints: the 1 year and 3 year mark. We divided patients into two categories: those labeled as &amp;rsquo;negative&amp;rsquo; if lung cancer was reported as their cause of death and their survival time was shorter than the predetermined endpoint, and &amp;lsquo;positive&amp;rsquo; if their survival time surpassed the specified endpoint. From here, we note the 1 year dataset is incredibly imbalanced, with only ~15% of patients dying before the clinical endpoint. The 3 year dataset has a more balanced split of about 55-45%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;samples.png&#34; alt=&#34;Clinical endpoint of samples&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;feature-selection&#34;&gt;Feature Selection&lt;/h2&gt;
&lt;p&gt;We look to perform feature selection on the gene expression data. Using all 20000 genes, DESeq2 was performed to obtain the top 100 genes associated with predictions. From here, these genes as well as several metadata features (age at diagnosis, gender, tumor stage, and smoker status) were run through XGBoost to determine the most useful features.&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;After training our data using XGBoost, we analyzed the importance of each feature. Using a significance threshold, features above this threshold were retained for classification. Classification was performed using XGBoost as well as Support Vector Machines (SVM) using a grid search approach. Due to the imbalanced dataset, we train our models based on F1 score.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;In the 1 year approach, we were able to obtain a maximal F1 score of 0.6 for the dead patient group. We note that the accuracy metric is misleading, since there are significantly more alive than dead patients.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;results_1year.png&#34; alt=&#34;Results for 1 year&#34; width=&#34;800&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;In the 3 year approach, we were able to obtain a maximal classification accuracy of 69.05%. Since our dataset is more balanced, reporting accuracy is not as misleading as in the 1 year case.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;results_3year.png&#34; alt=&#34;Results for 3 years&#34; width=&#34;800&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;Overall, this project successfully displays that careful feature selection using bioinformatics and classical machine learning techniques can identify prognostic features towards predicting clinical endpoint.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This group project was inspired by ECE 204: Statistical Learning in Bioinformatics, taught at UC San Diego under Dr. Ludmil Alexandrov.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Approach to Motor Imagery Classification via Mini-Epoch Generation
</title>
      <link>https://darintsui.github.io/project/motorimagery_featureselection/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/project/motorimagery_featureselection/</guid>
      <description>&lt;p&gt;Classification of motor imagery tasks based on EEG signals has a wide range of applications in clinical science and Brain-Computer Interfaces (BCIs). However, most deep learning attempts at motor imagery classification fail to retain the temporal information in the EEG signals that is critical for motor imagery. Here, we propose two algorithms, mini-epoch learning and mini-epoch ensemble learning, that aim to tackle this limitation.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the Berlin Brain-Computer Interface (BCI) Competition IV &lt;a href=&#34;https://www.bbci.de/competition/iv/#dataset2a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; 2a. In this, we have subjects performing motor imagery tasks. Motor imagery is a cognitive process in where a subject imagines performing a movement without moving their body. In this experimental setup, subjects were asked to imagine four movements: moving the left hand, right hand, tongue, and foot.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;timing_scheme.png&#34; alt=&#34;Timing scheme&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;feature-selection&#34;&gt;Feature Selection&lt;/h2&gt;
&lt;p&gt;We pull relevant information from the EEG signals by
extracting the data 1 second to 4 seconds after the stimulus
was cued. From here, we divide the input signals in the
time domain into small sub-intervals, which we will refer to
as “mini-epochs”. We experimented with two mini-epoch
generation strategies: fixed and sliding.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fixed mini-epoch window: The time signal is divided
into N non-overlapping intervals with equal length. In
this approach, the next interval starts from the point
where the first interval ends as shown below. Each
mini-epoch spans T = 3 N seconds&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;interval.png&#34; alt=&#34;Fixed window&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Sliding mini-epoch window: With fixed window
length T second of each epoch and the number of mini-
epochs N determined, the mini-epoch can be gener-
ated by sliding the window by increment of ∆t = (3−T)/(N−1)
seconds. Using this approach, we can more flexibly
determine the mini-epoch duration and the number of
mini-epochs while spanning the full time sequence via
varying the parameters T and N . These mini-epochs
can be overlapping as shown in Figure 3. Using this
approach, we can more clearly observe the effect of trimming the EEG signal into sub-intervals and look
for an optimal mini-epoch length that extracts the significance of different time steps for the EEG signal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;sliding.png&#34; alt=&#34;Sliding window&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;In our approach, after adding our mini-epoch window implementation, we follow the below pipeline.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Filter the signal through a bandpass filter from 7Hz to
30Hz, which is the only frequency range containing
relevant information about the motor imagery tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove signals from EOG channels and consider only
the 22 EEG channels for motor imagery tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply wavelet packet decomposition (WPD) tech-
nique to the resulting signal to extract features in the
frequency domain. WPD is an extension of Wavelet
Decomposition that comprises several bases with more
filtering operations applied to the wavelets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply Common Spatial Pattern (CSP) technique for
extracting spatial features from the signal. It has been
widely used for feature extraction in EEG-based BCI
systems for motor imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;preproc.png&#34; alt=&#34;Signal processing pipeline&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;learning-architectures&#34;&gt;Learning Architectures&lt;/h2&gt;
&lt;p&gt;We explore two learning architectures: mini-epoch learning and mini-epoch ensemble learning.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Mini-epoch learning&lt;/strong&gt;: After signal processing is applied to our data, we create
a feature set out of the resulting signals. From here, this
feature set is fed into a classifier to output the most probable class out of the entire time series. This classifier can be any classification algorithm, such as multi-layer perception (MLP), XGBoost, and support vector machine (SVM).&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;Mini-epoch learning&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Mini-epoch ensemble learning&lt;/strong&gt;: In this implementation, after signal processing is applied
to our data, we feed the resulting signals to a selected base
multi-class classifier. However, instead of directly yielding
the predicted label, these base classifiers are employed to
generate the normalized scores for each label. These nor-
malized scores indicate the probability distribution of each
label predicted within each mini-epoch. Like the classifier
in the mini-epoch learning algorithm, the base classifiers
can be implemented with any classification algorithm. These N base classifiers form the ensemble to make the
final prediction for the last step.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;mini_epoch_architecture.png&#34; alt=&#34;Mini-epoch ensemble learning&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;We look to test our feature selection technique using three classifiers: multi-layer perceptron (MLP), XGBoost, and SVM. We find that implementing our mini-epoch learning algorithm with SVM proved to be the most accurate in identifying different motor imagery tasks, with an accuracy of 63.16%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;mini_epoch_table.png&#34; alt=&#34;Table 1&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;Implementing mini-epoch ensemble learning with XGB recorded the highest accuracy of 61.04%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;ensemble_table.png&#34; alt=&#34;Table 1&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This group project was inspired by ECE 271B: Statistical Learning II, taught at UC San Diego under Dr. Manuela Vasconcelos.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://darintsui.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning: A Gentle Introduction to Image Classification
</title>
      <link>https://darintsui.github.io/post/imageclassification/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/post/imageclassification/</guid>
      <description>&lt;p&gt;This project and codebase was created for a deep learning workshop for the Institute of Electrical and Electronics Engineers (IEEE) @ UCSD. Here, we go over the basics of image classification using convolutional neural networks (CNN) on the MNIST dataset.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/mnist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MNIST dataset&lt;/a&gt;. In this set, we are given images comprised of handwritten digits from 0 to 9.&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;We format the data by one-hot encoding the labels from 0-9. We then look to normalize our data by dividing the pixel data information by 255. From here, we split our data into train and testing sets, and train a basic CNN model while varying common parameters such as learning rate, batch size, number of epochs, and optimizer. Using this implementation, we are able to obtain an accuracy of 98.77%.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;For a more thorough implementation of CNN models on a harder dataset, feel free to check out my implementation of AlexNet on plankton classification!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A machine learning approach to COVID-19 detection via Graphene Field-Effect-Transistor (GFET)</title>
      <link>https://darintsui.github.io/publication/gfet/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/publication/gfet/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>An optical tracking approach to computer-assisted surgical navigation via stereoscopic vision</title>
      <link>https://darintsui.github.io/publication/stereonavigation/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/publication/stereonavigation/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Implementation and Optimization of Convolutional Neural Networks for Plankton Classification
</title>
      <link>https://darintsui.github.io/project/planktonclassification/</link>
      <pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/project/planktonclassification/</guid>
      <description>&lt;p&gt;In order to process the large amounts of image data collected by underwater imaging sensors, we evaluated the usage of convolutional neural networks to perform image classification. We preprocessed the image data and trained CNN models 121 different classes of plankton.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the &lt;a href=&#34;https://www.kaggle.com/c/datasciencebowl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2014 National Data Science Bowl&lt;/a&gt;. In this set, we are given images spanning 121 different classes of plankton. In addition, our dataset comes highly imbalanced.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-and-optimization&#34;&gt;Preprocessing and Optimization&lt;/h2&gt;
&lt;p&gt;Using the image data, we resize all images to be 48 x 48. We also nrmalize the RGB values of the images in accordance with their relative weight in grayscale. From here, we try several techniques to improve the accuracy of our classifier: varying learning rate, varying optimizer, implementing Leaky ReLU, implementing cross-validation, and augmenting data via affine transformation.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;parameters.png&#34; alt=&#34;Default parameters used&#34; width=&#34;400&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;Using the data, we look to train and test our classifier using the AlexNet implementation from PyTorch. After hypertuning our parameters, we were able to obtain a classification accuracy of 74%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;AlexNet.png&#34; alt=&#34;AlexNet architecture&#34; width=&#34;200&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This group project was inspired by COGS 181: Neural Networks and Deep Learning, taught at UC San Diego under Dr. Zhuowen Tu.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cracking the Code - Machine Learning and Medical Diagnosis
</title>
      <link>https://darintsui.github.io/post/crackingthecode/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/post/crackingthecode/</guid>
      <description>&lt;p&gt;This project and codebase was created for a machine learning workshop for the Biomedical Engineering Society (BMES) at UC San Diego. Here, we go over the basics of image classification in MRI imaging.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from &lt;a href=&#34;https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt;. In this set, we are given images comprised of yes and no labels.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;yes.png&#34; alt=&#34;Yes label&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;We preprocess the data by extracting the RGB values of the images after resizing. From here, we perform K-Nearest Neighbors (KNN) over a wide range of neighbors using 5-fold cross-validation. Using this approach, we are able to obtain an accuracy of 77.49% at 73 nearest neighbors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hacking the Brain: Machine Learning and Human Behavior
</title>
      <link>https://darintsui.github.io/post/hackingthebrain/</link>
      <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/post/hackingthebrain/</guid>
      <description>&lt;p&gt;This project and codebase was created for a computational neuroscience workshop for the Institute of Electrical and Electronics Engineers (IEEE) @ UCSD. Here, we go over the basics of supervised learning in electroencephalogram (EEG) data.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the Berlin Brain-Computer Interface (BCI) Competition IV &lt;a href=&#34;https://www.bbci.de/competition/iv/#dataset2a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; 2a. In this, we have subjects performing motor imagery tasks. Motor imagery is a cognitive process in where a subject imagines performing a movement without moving their body. In this experimental setup, subjects were asked to imagine four movements: moving the left hand, right hand, tongue, and foot.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;experimental_setup.png&#34; alt=&#34;Timing scheme&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;feature-selection&#34;&gt;Feature Selection&lt;/h2&gt;
&lt;p&gt;We perform feature selection by analyzing the EEG data usinga biological lens. We extract EEG data from the somatosensory cortex within the beta frequency band (8-30 Hz). We&amp;rsquo;re able to extract these features in the frequency domain by transforming our data to the power spectral density (PSD). For demonstration purposes, we look to perform binary classification on the left hand and foot.&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;From here, we perform common spatial patterns (CSP), followed by linear discriminant analysis (LDA). Using this method, we obtain an accuracy of 76.55%.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;For a more thorough analysis of the feasibility of multi-class EEG classification, you can refer to my motor imagery project developing a novel feature selection and classification pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optical surgical navigation: a promising low-cost alternative</title>
      <link>https://darintsui.github.io/publication/opticalnavigation/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/publication/opticalnavigation/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://darintsui.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} One {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} Three {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>cv</title>
      <link>https://darintsui.github.io/cv/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/cv/</guid>
      <description>&lt;h1 id=&#34;darin-tsui&#34;&gt;Darin Tsui&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Email&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:dtsui@ieee.org&#34;&gt;dtsui@ieee.org&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linkedin&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/darintsui/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/in/darintsui/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GitHub&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/darintsui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/darintsui&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;../uploads/DarinTsui_CV.pdf&#34;&gt;CV&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;../uploads/DarinTsui_Resume.pdf&#34;&gt;Resume&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;span-stylefont-size24pxeducationspan&#34;&gt;&lt;span style=&#34;font-size:24px;&#34;&gt;Education&lt;/span&gt;&lt;/h2&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;span style=&#34;font-size:20px;&#34;&gt;&lt;strong&gt;Ph.D. in Electrical and Computer Engineering&lt;/strong&gt; &lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span style=&#34;font-size:20px;&#34;&gt;Georgia Institute of Technology &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;&lt;strong&gt;Advisor:&lt;/strong&gt; &lt;a href=&#34;https://amirmohan.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amirali Aghazadeh&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;Aug. 2023 - Present &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;dt&gt;&lt;span style=&#34;font-size:20px;&#34;&gt;&lt;strong&gt;B.S. in Bioengineering&lt;/strong&gt; &lt;/span&gt;&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;span style=&#34;font-size:20px;&#34;&gt;University of California San Diego &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;&lt;strong&gt;Affiliates:&lt;/strong&gt; &lt;a href=&#34;https://isn.ucsd.edu/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gert Cauwenberghs&lt;/a&gt; and &lt;a href=&#34;https://www.talkelab.ucsd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank E. Talke&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;Sept. 2019 - Jun. 2023 &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;hr&gt;
&lt;h2 id=&#34;span-stylefont-size24pxawardsspan&#34;&gt;&lt;span style=&#34;font-size:24px;&#34;&gt;Awards&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-size:20px;&#34;&gt;&lt;strong&gt;Anushka Michailova Memorial Best Undergraduate Poster Award&lt;/strong&gt; &lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;&lt;strong&gt;Poster:&lt;/strong&gt; COVID-19 Detection via. Graphene Field-Effect-Transistor (GFET) &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;font-size:18px;&#34;&gt;May 2023&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://darintsui.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
