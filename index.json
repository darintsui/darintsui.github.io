
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hi there! My name is Darin. I’m an incoming PhD student at the Georgia Institute of Technology studying Electrical and Computer Engineering. I previously received my BS degree at the University of California San Diego in Bioengineering. I am currently working on developing artifical intelligence solutions in applications of bioinformatics with Dr. Amirali Aghazadeh. In my undergrad, I was designing a low-cost surgical navigation platform using computer vision under Dr. Frank Talke in the Talke Biomedical Device Lab. I have also explored applications of machine learning in bioelectronic COVID-19 detection under Dr. Gert Cauwenberghs in the Integrated Systems Neuroengineering Laboratory. Outside of research, I was the 2022-2023 President of the Institute of Electrical and Electronics Engineers (IEEE) student branch at UC San Diego. I am an active IEEE member as well as a member of the IEEE Engineering in Medicine and Biology Society (EMBS).\n","date":1682899200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1682899200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi there! My name is Darin. I’m an incoming PhD student at the Georgia Institute of Technology studying Electrical and Computer Engineering. I previously received my BS degree at the University of California San Diego in Bioengineering.","tags":null,"title":"Darin Tsui","type":"authors"},{"authors":null,"categories":null,"content":"“How long have I got?”\nMost Stage IV lung cancer patients worldwide tend to have poor survival: 25-30% die within the first 3 months of diagnosis. However, of those patients suriving longer than 3 months, 10-15% of those patients survive very long. The question becomes then: what metrics can we use to determine a patient’s survival projection using clinically relevant data. In this project, we look to develop and validate a survival prediction model in a cohort of different stages patients of lung cancer.\nData Selection Data for this project was taken from the Cancer Genome Atlas Lung Adenocarcinoma (TCGA-LUAD). The data consists of 572 samples obtained from 512 individuals, with over 20,000 genes for each. We mainly utilize the gene expression and metadata datasets for our analysis. We performed Log2 transformation on the expression data and removed the recurrent and normal samples.\nWe look to analyze two different clinical endpoints: the 1 year and 3 year mark. We divided patients into two categories: those labeled as ’negative’ if lung cancer was reported as their cause of death and their survival time was shorter than the predetermined endpoint, and ‘positive’ if their survival time surpassed the specified endpoint. From here, we note the 1 year dataset is incredibly imbalanced, with only ~15% of patients dying before the clinical endpoint. The 3 year dataset has a more balanced split of about 55-45%.\nFeature Selection We look to perform feature selection on the gene expression data. Using all 20000 genes, DESeq2 was performed to obtain the top 100 genes associated with predictions. From here, these genes as well as several metadata features (age at diagnosis, gender, tumor stage, and smoker status) were run through XGBoost to determine the most useful features.\nTraining and Testing After training our data using XGBoost, we analyzed the importance of each feature. Using a significance threshold, features above this threshold were retained for classification. Classification was performed using XGBoost as well as Support Vector Machines (SVM) using a grid search approach. Due to the imbalanced dataset, we train our models based on F1 score.\nResults In the 1 year approach, we were able to obtain a maximal F1 score of 0.6 for the dead patient group. We note that the accuracy metric is misleading, since there are significantly more alive than dead patients.\nIn the 3 year approach, we were able to obtain a maximal classification accuracy of 69.05%. Since our dataset is more balanced, reporting accuracy is not as misleading as in the 1 year case.\nOverall, this project successfully displays that careful feature selection using bioinformatics and classical machine learning techniques can identify prognostic features towards predicting clinical endpoint.\nAcknowledgements This group project was inspired by ECE 204: Statistical Learning in Bioinformatics, taught at UC San Diego under Dr. Ludmil Alexandrov.\n","date":1686700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686700800,"objectID":"0f322cd1114cb62c17bcbf178a6992fe","permalink":"https://darintsui.github.io/project/cancersurvival/","publishdate":"2023-06-14T00:00:00Z","relpermalink":"/project/cancersurvival/","section":"project","summary":"DESeq2 and XGBoost feature selection on gene expression and metadata.","tags":["Bioinformatics"],"title":"Supervised ML Approaches for Predicting Cancer Survival Rate","type":"project"},{"authors":null,"categories":null,"content":"Classification of motor imagery tasks based on EEG signals has a wide range of applications in clinical science and Brain-Computer Interfaces (BCIs). However, most deep learning attempts at motor imagery classification fail to retain the temporal information in the EEG signals that is critical for motor imagery. Here, we propose two algorithms, mini-epoch learning and mini-epoch ensemble learning, that aim to tackle this limitation.\nData Selection Data for this project was taken from the Berlin Brain-Computer Interface (BCI) Competition IV dataset 2a. In this, we have subjects performing motor imagery tasks. Motor imagery is a cognitive process in where a subject imagines performing a movement without moving their body. In this experimental setup, subjects were asked to imagine four movements: moving the left hand, right hand, tongue, and foot.\nFeature Selection We pull relevant information from the EEG signals by extracting the data 1 second to 4 seconds after the stimulus was cued. From here, we divide the input signals in the time domain into small sub-intervals, which we will refer to as “mini-epochs”. We experimented with two mini-epoch generation strategies: fixed and sliding.\nFixed mini-epoch window: The time signal is divided into N non-overlapping intervals with equal length. In this approach, the next interval starts from the point where the first interval ends as shown below. Each mini-epoch spans T = 3 N seconds Sliding mini-epoch window: With fixed window length T second of each epoch and the number of mini- epochs N determined, the mini-epoch can be gener- ated by sliding the window by increment of ∆t = (3−T)/(N−1) seconds. Using this approach, we can more flexibly determine the mini-epoch duration and the number of mini-epochs while spanning the full time sequence via varying the parameters T and N . These mini-epochs can be overlapping as shown in Figure 3. Using this approach, we can more clearly observe the effect of trimming the EEG signal into sub-intervals and look for an optimal mini-epoch length that extracts the significance of different time steps for the EEG signal. Preprocessing In our approach, after adding our mini-epoch window implementation, we follow the below pipeline.\nFilter the signal through a bandpass filter from 7Hz to 30Hz, which is the only frequency range containing relevant information about the motor imagery tasks.\nRemove signals from EOG channels and consider only the 22 EEG channels for motor imagery tasks.\nApply wavelet packet decomposition (WPD) tech- nique to the resulting signal to extract features in the frequency domain. WPD is an extension of Wavelet Decomposition that comprises several bases with more filtering operations applied to the wavelets.\nApply Common Spatial Pattern (CSP) technique for extracting spatial features from the signal. It has been widely used for feature extraction in EEG-based BCI systems for motor imagery.\nLearning Architectures We explore two learning architectures: mini-epoch learning and mini-epoch ensemble learning.\nMini-epoch learning: After signal processing is applied to our data, we create a feature set out of the resulting signals. From here, this feature set is fed into a classifier to output the most probable class out of the entire time series. This classifier can be any classification algorithm, such as multi-layer perception (MLP), XGBoost, and support vector machine (SVM). Mini-epoch ensemble learning: In this implementation, after signal processing is applied to our data, we feed the resulting signals to a selected base multi-class classifier. However, instead of directly yielding the predicted label, these base classifiers are employed to generate the normalized scores for each label. These nor- malized scores indicate the probability distribution of each label predicted within each mini-epoch. Like the classifier in the mini-epoch learning algorithm, the base classifiers can be implemented with any classification algorithm. These N base classifiers form the ensemble to make the final prediction for the last step. Training and Testing We look to test our feature selection technique using three classifiers: multi-layer perceptron (MLP), XGBoost, and SVM. We find that implementing our mini-epoch learning algorithm with SVM proved to be the most accurate in identifying different motor imagery tasks, with an accuracy of 63.16%.\nImplementing mini-epoch ensemble learning with XGB recorded the highest accuracy of 61.04%.\nAcknowledgements This group project was inspired by ECE 271B: Statistical Learning II, taught at UC San Diego under Dr. Manuela Vasconcelos.\n","date":1679011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679011200,"objectID":"6139b0a2078bc9f250227559688e4c20","permalink":"https://darintsui.github.io/project/motorimagery_featureselection/","publishdate":"2023-03-17T00:00:00Z","relpermalink":"/project/motorimagery_featureselection/","section":"project","summary":"We propose two feature selection algorithms, mini-epoch learning and mini-epoch ensemble learning, that aim to capture temporal data in EEG for motor imagery.","tags":["Deep Learning"],"title":"A Novel Approach to Motor Imagery Classification via Mini-Epoch Generation\n","type":"project"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://darintsui.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"This project and codebase was created for a deep learning workshop for the Institute of Electrical and Electronics Engineers (IEEE) @ UCSD. Here, we go over the basics of image classification using convolutional neural networks (CNN) on the MNIST dataset.\nData Selection Data for this project was taken from MNIST dataset. In this set, we are given images comprised of handwritten digits from 0 to 9.\nTraining and Testing We format the data by one-hot encoding the labels from 0-9. We then look to normalize our data by dividing the pixel data information by 255. From here, we split our data into train and testing sets, and train a basic CNN model while varying common parameters such as learning rate, batch size, number of epochs, and optimizer. Using this implementation, we are able to obtain an accuracy of 98.77%.\nFuture Work For a more thorough implementation of CNN models on a harder dataset, feel free to check out my implementation of AlexNet on plankton classification!\n","date":1684022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684022400,"objectID":"86403f93299f07a6ff4c6743048507c2","permalink":"https://darintsui.github.io/post/imageclassification/","publishdate":"2023-05-14T00:00:00Z","relpermalink":"/post/imageclassification/","section":"post","summary":"Provides a tutorial on convolutional neural networks (CNN) on the MNIST dataset.","tags":["Teaching","Deep Learning"],"title":"Machine Learning: A Gentle Introduction to Image Classification\n","type":"post"},{"authors":["Darin Tsui","Francisco Downey","Shreenithi Navaneethan","Akshay Paul","Tyler Bodily","Min Lee","Yuchen Xu","Ratnesh Lal","Gert Cauwenberghs"],"categories":null,"content":"\r","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"74cc42fd61f6291b02db189b60261ce8","permalink":"https://darintsui.github.io/publication/gfet/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/gfet/","section":"publication","summary":"We evaluate that the usage of a graphene field-effect-transistor (GFET) coupled with machine learning can be a promising alternate diagnostic testing method in COVID-19 detection.","tags":[],"title":"A machine learning approach to COVID-19 detection via Graphene Field-Effect-Transistor (GFET)","type":"publication"},{"authors":["Darin Tsui","Capalina Melentyev","Ananya Rajan","Rohan Kumar","Frank E. Talke"],"categories":null,"content":"\r","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"62798fa78932d98b567edc3a53cb6002","permalink":"https://darintsui.github.io/publication/stereonavigation/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/stereonavigation/","section":"publication","summary":"We propose that fully optical navigation has the potential to be a viable alternative to state-of-the-art reflective marker navigation.","tags":[],"title":"An optical tracking approach to computer-assisted surgical navigation via stereoscopic vision","type":"publication"},{"authors":null,"categories":null,"content":"In order to process the large amounts of image data collected by underwater imaging sensors, we evaluated the usage of convolutional neural networks to perform image classification. We preprocessed the image data and trained CNN models 121 different classes of plankton.\nData Selection Data for this project was taken from the 2014 National Data Science Bowl. In this set, we are given images spanning 121 different classes of plankton. In addition, our dataset comes highly imbalanced.\nPreprocessing and Optimization Using the image data, we resize all images to be 48 x 48. We also nrmalize the RGB values of the images in accordance with their relative weight in grayscale. From here, we try several techniques to improve the accuracy of our classifier: varying learning rate, varying optimizer, implementing Leaky ReLU, implementing cross-validation, and augmenting data via affine transformation.\nTraining and Testing Using the data, we look to train and test our classifier using the AlexNet implementation from PyTorch. After hypertuning our parameters, we were able to obtain a classification accuracy of 74%.\nAcknowledgements This group project was inspired by COGS 181: Neural Networks and Deep Learning, taught at UC San Diego under Dr. Zhuowen Tu.\n","date":1680652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680652800,"objectID":"7b6d553641644a8f1793ac405b0feaf1","permalink":"https://darintsui.github.io/project/planktonclassification/","publishdate":"2023-04-05T00:00:00Z","relpermalink":"/project/planktonclassification/","section":"project","summary":"Implementation of AlexNet for plankton classification.","tags":["Deep Learning"],"title":"Implementation and Optimization of Convolutional Neural Networks for Plankton Classification\n","type":"project"},{"authors":null,"categories":null,"content":"This project and codebase was created for a machine learning workshop for the Biomedical Engineering Society (BMES) at UC San Diego. Here, we go over the basics of image classification in MRI imaging.\nData Selection Data for this project was taken from Kaggle. In this set, we are given images comprised of yes and no labels.\nTraining and Testing We preprocess the data by extracting the RGB values of the images after resizing. From here, we perform K-Nearest Neighbors (KNN) over a wide range of neighbors using 5-fold cross-validation. Using this approach, we are able to obtain an accuracy of 77.49% at 73 nearest neighbors.\n","date":1677456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677456000,"objectID":"f05c9f73c7b6e7bddf991085d822d3da","permalink":"https://darintsui.github.io/post/crackingthecode/","publishdate":"2023-02-27T00:00:00Z","relpermalink":"/post/crackingthecode/","section":"post","summary":"Provides a tutorial on image classification using MRI data, as well as an overview of machine learning in biomedical engineering.","tags":["Teaching"],"title":"Cracking the Code - Machine Learning and Medical Diagnosis\n","type":"post"},{"authors":null,"categories":null,"content":"This project and codebase was created for a computational neuroscience workshop for the Institute of Electrical and Electronics Engineers (IEEE) @ UCSD. Here, we go over the basics of supervised learning in electroencephalogram (EEG) data.\nData Selection Data for this project was taken from the Berlin Brain-Computer Interface (BCI) Competition IV dataset 2a. In this, we have subjects performing motor imagery tasks. Motor imagery is a cognitive process in where a subject imagines performing a movement without moving their body. In this experimental setup, subjects were asked to imagine four movements: moving the left hand, right hand, tongue, and foot.\nFeature Selection We perform feature selection by analyzing the EEG data usinga biological lens. We extract EEG data from the somatosensory cortex within the beta frequency band (8-30 Hz). We’re able to extract these features in the frequency domain by transforming our data to the power spectral density (PSD). For demonstration purposes, we look to perform binary classification on the left hand and foot.\nTraining and Testing From here, we perform common spatial patterns (CSP), followed by linear discriminant analysis (LDA). Using this method, we obtain an accuracy of 76.55%.\nFuture Work For a more thorough analysis of the feasibility of multi-class EEG classification, you can refer to my motor imagery project developing a novel feature selection and classification pipeline.\n","date":1675641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675641600,"objectID":"6ff5bb338dbb97625c130a674c337398","permalink":"https://darintsui.github.io/post/hackingthebrain/","publishdate":"2023-02-06T00:00:00Z","relpermalink":"/post/hackingthebrain/","section":"post","summary":"Provides an overview of the commputational neuroscience field, and applies signal processing and physiological-based approaches to motor imagery classification.","tags":["Teaching"],"title":"Hacking the Brain: Machine Learning and Human Behavior\n","type":"post"},{"authors":["Darin Tsui","Mitsuhiro Jo","Bryan Nguyen","Farshad Ahadian","Frank E. Talke"],"categories":null,"content":"\r","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"65ef1df9354279512ce7b8df84321496","permalink":"https://darintsui.github.io/publication/opticalnavigation/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/opticalnavigation/","section":"publication","summary":"We find that our navigation system is a promising approach for use in computer-navigated surgery, and future work will focus on implementing image processing techniques to improve the accuracy of optical marker tracking.","tags":[],"title":"Optical surgical navigation: a promising low-cost alternative","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://darintsui.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Darin Tsui Email dtsui@ieee.org Linkedin https://www.linkedin.com/in/darintsui/ GitHub https://github.com/darintsui CV Resume Education Ph.D. in Electrical and Computer Engineering Georgia Institute of Technology Advisor: Amirali Aghazadeh Aug. 2023 - Present B.S. in Bioengineering University of California San Diego Affiliates: Gert Cauwenberghs and Frank E. Talke Sept. 2019 - Jun. 2023 Awards Anushka Michailova Memorial Best Undergraduate Poster Award Poster: COVID-19 Detection via. Graphene Field-Effect-Transistor (GFET) May 2023 ","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://darintsui.github.io/cv/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"cv","type":"page"}]