<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Darin Tsui</title>
    <link>https://darintsui.github.io/tag/deep-learning/</link>
      <atom:link href="https://darintsui.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 14 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://darintsui.github.io/media/icon_hu382b23d3a754463455488130ff2cf495_259114_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://darintsui.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>A Novel Approach to Motor Imagery Classification via Mini-Epoch Generation
</title>
      <link>https://darintsui.github.io/project/motorimagery_featureselection/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/project/motorimagery_featureselection/</guid>
      <description>&lt;p&gt;Classification of motor imagery tasks based on EEG signals has a wide range of applications in clinical science and Brain-Computer Interfaces (BCIs). However, most deep learning attempts at motor imagery classification fail to retain the temporal information in the EEG signals that is critical for motor imagery. Here, we propose two algorithms, mini-epoch learning and mini-epoch ensemble learning, that aim to tackle this limitation.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the Berlin Brain-Computer Interface (BCI) Competition IV &lt;a href=&#34;https://www.bbci.de/competition/iv/#dataset2a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; 2a. In this, we have subjects performing motor imagery tasks. Motor imagery is a cognitive process in where a subject imagines performing a movement without moving their body. In this experimental setup, subjects were asked to imagine four movements: moving the left hand, right hand, tongue, and foot.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;timing_scheme.png&#34; alt=&#34;Timing scheme&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;feature-selection&#34;&gt;Feature Selection&lt;/h2&gt;
&lt;p&gt;We pull relevant information from the EEG signals by
extracting the data 1 second to 4 seconds after the stimulus
was cued. From here, we divide the input signals in the
time domain into small sub-intervals, which we will refer to
as “mini-epochs”. We experimented with two mini-epoch
generation strategies: fixed and sliding.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fixed mini-epoch window: The time signal is divided
into N non-overlapping intervals with equal length. In
this approach, the next interval starts from the point
where the first interval ends as shown below. Each
mini-epoch spans T = 3 N seconds&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;interval.png&#34; alt=&#34;Fixed window&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Sliding mini-epoch window: With fixed window
length T second of each epoch and the number of mini-
epochs N determined, the mini-epoch can be gener-
ated by sliding the window by increment of ∆t = (3−T)/(N−1)
seconds. Using this approach, we can more flexibly
determine the mini-epoch duration and the number of
mini-epochs while spanning the full time sequence via
varying the parameters T and N . These mini-epochs
can be overlapping as shown in Figure 3. Using this
approach, we can more clearly observe the effect of trimming the EEG signal into sub-intervals and look
for an optimal mini-epoch length that extracts the significance of different time steps for the EEG signal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;sliding.png&#34; alt=&#34;Sliding window&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;In our approach, after adding our mini-epoch window implementation, we follow the below pipeline.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Filter the signal through a bandpass filter from 7Hz to
30Hz, which is the only frequency range containing
relevant information about the motor imagery tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove signals from EOG channels and consider only
the 22 EEG channels for motor imagery tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply wavelet packet decomposition (WPD) tech-
nique to the resulting signal to extract features in the
frequency domain. WPD is an extension of Wavelet
Decomposition that comprises several bases with more
filtering operations applied to the wavelets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply Common Spatial Pattern (CSP) technique for
extracting spatial features from the signal. It has been
widely used for feature extraction in EEG-based BCI
systems for motor imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;preproc.png&#34; alt=&#34;Signal processing pipeline&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;learning-architectures&#34;&gt;Learning Architectures&lt;/h2&gt;
&lt;p&gt;We explore two learning architectures: mini-epoch learning and mini-epoch ensemble learning.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Mini-epoch learning&lt;/strong&gt;: After signal processing is applied to our data, we create
a feature set out of the resulting signals. From here, this
feature set is fed into a classifier to output the most probable class out of the entire time series. This classifier can be any classification algorithm, such as multi-layer perception (MLP), XGBoost, and support vector machine (SVM).&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;Mini-epoch learning&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Mini-epoch ensemble learning&lt;/strong&gt;: In this implementation, after signal processing is applied
to our data, we feed the resulting signals to a selected base
multi-class classifier. However, instead of directly yielding
the predicted label, these base classifiers are employed to
generate the normalized scores for each label. These nor-
malized scores indicate the probability distribution of each
label predicted within each mini-epoch. Like the classifier
in the mini-epoch learning algorithm, the base classifiers
can be implemented with any classification algorithm. These N base classifiers form the ensemble to make the
final prediction for the last step.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;mini_epoch_architecture.png&#34; alt=&#34;Mini-epoch ensemble learning&#34; width=&#34;700&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;We look to test our feature selection technique using three classifiers: multi-layer perceptron (MLP), XGBoost, and SVM. We find that implementing our mini-epoch learning algorithm with SVM proved to be the most accurate in identifying different motor imagery tasks, with an accuracy of 63.16%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;mini_epoch_table.png&#34; alt=&#34;Table 1&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;p&gt;Implementing mini-epoch ensemble learning with XGB recorded the highest accuracy of 61.04%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;ensemble_table.png&#34; alt=&#34;Table 1&#34; width=&#34;600&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This group project was inspired by ECE 271B: Statistical Learning II, taught at UC San Diego under Dr. Manuela Vasconcelos.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning: A Gentle Introduction to Image Classification
</title>
      <link>https://darintsui.github.io/post/imageclassification/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/post/imageclassification/</guid>
      <description>&lt;p&gt;This project and codebase was created for a deep learning workshop for the Institute of Electrical and Electronics Engineers (IEEE) @ UCSD. Here, we go over the basics of image classification using convolutional neural networks (CNN) on the MNIST dataset.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/mnist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MNIST dataset&lt;/a&gt;. In this set, we are given images comprised of handwritten digits from 0 to 9.&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;We format the data by one-hot encoding the labels from 0-9. We then look to normalize our data by dividing the pixel data information by 255. From here, we split our data into train and testing sets, and train a basic CNN model while varying common parameters such as learning rate, batch size, number of epochs, and optimizer. Using this implementation, we are able to obtain an accuracy of 98.77%.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;For a more thorough implementation of CNN models on a harder dataset, feel free to check out my implementation of AlexNet on plankton classification!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation and Optimization of Convolutional Neural Networks for Plankton Classification
</title>
      <link>https://darintsui.github.io/project/planktonclassification/</link>
      <pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://darintsui.github.io/project/planktonclassification/</guid>
      <description>&lt;p&gt;In order to process the large amounts of image data collected by underwater imaging sensors, we evaluated the usage of convolutional neural networks to perform image classification. We preprocessed the image data and trained CNN models 121 different classes of plankton.&lt;/p&gt;
&lt;h2 id=&#34;data-selection&#34;&gt;Data Selection&lt;/h2&gt;
&lt;p&gt;Data for this project was taken from the &lt;a href=&#34;https://www.kaggle.com/c/datasciencebowl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2014 National Data Science Bowl&lt;/a&gt;. In this set, we are given images spanning 121 different classes of plankton. In addition, our dataset comes highly imbalanced.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-and-optimization&#34;&gt;Preprocessing and Optimization&lt;/h2&gt;
&lt;p&gt;Using the image data, we resize all images to be 48 x 48. We also nrmalize the RGB values of the images in accordance with their relative weight in grayscale. From here, we try several techniques to improve the accuracy of our classifier: varying learning rate, varying optimizer, implementing Leaky ReLU, implementing cross-validation, and augmenting data via affine transformation.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;parameters.png&#34; alt=&#34;Default parameters used&#34; width=&#34;400&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;training-and-testing&#34;&gt;Training and Testing&lt;/h2&gt;
&lt;p&gt;Using the data, we look to train and test our classifier using the AlexNet implementation from PyTorch. After hypertuning our parameters, we were able to obtain a classification accuracy of 74%.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img src=&#34;AlexNet.png&#34; alt=&#34;AlexNet architecture&#34; width=&#34;200&#34;/&gt;
&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This group project was inspired by COGS 181: Neural Networks and Deep Learning, taught at UC San Diego under Dr. Zhuowen Tu.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
